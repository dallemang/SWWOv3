\chapter{Conclusions and Future Work}
\label{ch17}



For those readers who are accustomed to various sorts of knowledge
modeling, the Semantic Web looks familiar. The notions of classes,
subclasses, properties, and instances have been the mainstay of
knowledge modeling and object systems modeling for decades. It is not
uncommon to hear a veteran of one of these technologies look at the
Semantic Web and mutter, ''Same old, same old'' indicating that there
is nothing new going on here and that everything in the Semantic Web has
already been done under some other name elsewhere.

As the old saying goes, ``There is nothing new under the sun,'' and to
the extent that the saying is correct, so are these folks when they
speak of the Semantic Web. The modeling structures we have examined in
this book do have a strong connection to a heritage of knowledge
modeling languages. But there is something new that has come along since
the early days of expert systems and object-oriented programming;
something that has had a far more revolutionizing effect on culture,
business, commerce, education, and society than any expert system
designer ever dreamed of. It is something so revolutionary that it is
often compared in cultural significance to the invention of the printing
press. That something new is the World Wide Web.

The Semantic Web is the application of advanced technologies that have
been used in the context of artificial intelligence, expert systems and
business rules execution in the context of a World Wide Web of
information. The Semantic Web is not simply an application running on
the Web somewhere; it is a part of the very infrastructure of the Web.
It isn't on the Web; it is the Web.

Why is this important? What is it that is so special about the Web? Why
has it been so successful, more so than just about any computer system
that has come before it?

In the early days of the commercial Web, there was a television ad for a
search engine. In the ad, a woman driving a stylish sports car is pulled
over by traffic policeman for speeding. As he prepares to cite her, she
outlines for him all the statistics about error rates in the various
machines used by traffic policemen for detecting speeding. He is clearly
thrown off his game and unsure of how to continue to cite her. She adds
personal insult by quoting the statistics of prolonged exposure to
traffic radar machines on sperm count. The slogan ``Knowledge is Power''
scrolls over the screen, along with the name of the search engine.

What lesson can we learn from ads like this? This kind of advertising
made a break from television advertising that had come before. Knowledge
was seen not as nerdy or academic but useful in everyday life---and even
sexy. Or at least it is if you have the right knowledge at the right
time. The Web differed from information systems that preceded it by
bringing information from many sources---indeed, sources from around the
world---to one's fingertips. In comparison to Hypercard stacks that had
been around for decades, the Web was an open system. Anyone in the world
could contribute, and everyone could benefit from that contribution.
Having all that information available was more important than how well a
small amount of information was organized.

The Semantic Web differs from early knowledge-based and modeling systems in pretty much the same
way. It has been said that "a little semantics goes a long way."   That is, we don't need
a complex and elaborate semantic system and a huge knowledge base to benefit from semantics.  A 
simple semantic language, and  a small starter model, can provide great benefit in managing
a web of knowledge. 

Compared to the knowledge 
representations systems that came before the Web, OWL is quite
primitive, and RDFS even more so. But this is appropriate for  Web languages. The power of the
Semantic Web comes from its  Web aspect. Even a primitive knowledge
modeling language can yield impressive results when it uses information
from sources from around the world. In expert systems terms, the goals
of the Semantic Web are also modest. The idea of an expert system was
that it could behave in a problem-solving setting with a performance
that would qualify as expert-level if a human were to accomplish it.
What we learned from the World Wide Web (and the story of the woman
beating the speeding ticket) is that typically people don't want
machines to behave like experts; they want to have access to information
so they themselves can exhibit expert performance at just the right time. As we saw
in the ad, the World Wide Web was successful early on in making this
happen, as long as someone is willing to read the relevant web pages,
digest the information, and sift out what they need. 

The Semantic Web takes this idea one step further. The Web is effective
at bringing any single resource to the attention of a Web user, but if
the information the user needs is not represented in a single place, the
job of integration rests with the user. The Semantic Web doesn't use
expert system technology to replicate the behavior of an expert; it uses
expert system technology to gather information so an individual can have
integrated access to the web of information.

Being part of the Web infrastructure is no simple matter. On the Web,
any reference is a global reference. The issue of managing global names
for anything we want to talk about is a fundamental Web issue, not just
a Semantic Web issue. The Semantic Web uses the notion of an IRI as the
globally resolvable reference to a resource as a way of taking advantage
of the web infrastructure. Most programming and modeling languages have
a mechanism whereby names can be organized into spaces (so that you and
I can use the same name in different ways but still keep them straight
when our systems have to interface). 

The World Wide Web takes this idea to a global scale.  The notion of a name in a namespace must be
unique  in the entire Web.
The IRI is the Web-standard mechanism to do this; hence, the Semantic
Web uses the IRI for global namespace identification. Using this
approach allows the Semantic Web to borrow the modularity of the World
Wide Web. Two models that were developed in isolation can be merged
simply by referring to resources in both of them in the same statement.
Since the names are always maintained as global identifiers, there is no
ad hoc need to integrate identifiers each time; the system for global
identity is part of the infrastructure.

An important contributor to the success of the World Wide Web is its
openness. Anyone can contribute to the body of information, including
people who, for one reason or another, might publish information that
someone else would consider misleading, objectionable, or just
incorrect. At first blush, a chaotic free-for-all of this sort seems
insane. How could it ever be useful? The success of the Web in general
(and information archiving sites like Wikipedia in particular) has shown
that there is sufficient incentive to publish quality data to make the
overall Web a useful and even essential structure.

The Semantic Web standards in this book build on the structure of the 
hypertext web, allowing datasets to refer to one another using IRIs, thus 
forming a web of linked data.  This infrastucture supports a facility by which 
two or more datasets can reference the same identifier - thereby linking
the datasets together.  The RDF standard is based on the simplest possible 
linkage mechanism in a distributed data system; the ability to link 
one global referenced to another, with a named link (whose name is also a global reference). 
This global linked data Web is a natural extension of the hypertext Web 
we are all familiar with. 

This openness has serious ramifications for  the development of the Semantic Web infrastructure, which go
beyond considerations that were important for technologies like expert
systems. One of the reasons why the Web was more successful than
Hypercard was because the Web infrastructure was resilient to missing or
broken links (the ``404 Error''). The Semantic Web must be resilient in
a similar way. Thus, inferencing in the Semantic Web must be done very
conservatively, according to the Open World assumption. At any time, new
information could become available that could undermine conclusions that
have already been made, and our inference policy must be robust in such
situations.

In the World Wide Web, the openness of the system presents a potential
problem. How does the heroine of the search engine commercial know that
the information she has found about radar-based speed detection devices is correct? She might have learned it from a
trusted source (say, a government study on these devices), or she might
have cross-referenced the information with other sources until she had
enough corroborating evidence to be certain. Or perhaps she doesn't
really care if it is correct but only that she can convince the traffic
cop that it is. Trust of information on the Web is done with a healthy
dose of skepticism but in the same way as trust in other media like
newspapers, books, and magazine articles.

In this book, we examined the modeling aspects of the Semantic Web: How
do you represent information in such a way that it is responsive to a
web environment? The basic principles underlying the Semantic Web---the
AAA slogan, the Nonunique Naming assumption, and the Open World
assumption---are constraints placed on a representation system if it
wants to function as the foundation of a World Wide Web of information.
These constraints have led to the main design decisions for the Semantic
Web languages of RDF, RDFS, and OWL.

There is more to a web than just the information and how it is modeled.
At some point, this information must be stored in a computer, accessed
by end users, and transmitted across an information network.
Furthermore, no triple store, and no inference engine, will ever be able
to scale to the size of the World Wide Semantic Web. This is clearly
impossible, since the Web itself grows continually. In the light of this
observation, how can the World Wide Semantic Web ever come to pass?

The applications we discussed in this book demonstrate how a modest
amount of information, represented flexibly so that it can be merged in
novel ways, provides a new dynamic for information distribution and
sharing. SKOS allows thesaurus managers around the globe to share,
connect, and compare terminology. QUDT aligns multiple applications so
that their measurable quantities can be combined and compared. OBO
Ontologies coordinate efforts of independent life sciences researchers
around the globe.

How is it possible to get the benefit of a global network of data if no
machine is powerful enough to
store, inference over, and query the whole network? As we have seen, it
isn't necessary that a Semantic Web application be able to access and
merge every page on the Web at once. The Semantic Web is useful as long
as an application can access and merge any web page. Since we can't hold
all the Semantic Web pages in one store at once, we have to proceed with
the understanding that there could always be more information that we
don't have access to at any one point. This is why the Open World
assumption is central to the infrastructure of the Semantic Web.

This book is about modeling in the context of the Semantic Web. What
role does a model play in the big vision? The World Wide Web that we see
every day is made up primarily of hypertext web pages, which are read and digested
by people browsing the Web. But behind many of these web pages, there
are databases that contain far more information than is actually
displayed on a page. To make all this information available as a global,
integrated whole, we need a way to specify how information in one place
relates to information somewhere else. Models on the Semantic Web play
the role of the intermediaries that describe the relationships among
information from various sources.

Look at the cover of this book. An engineering handbook for aquifers
provides information about conduits, ducts, and channels sufficient to
inform an engineer about the pieces of a dynamic fluid system that can
control a series of waterways like these. The handbook won't give final
designs, but it will provide insight about how the pieces can be fit
together to accomplish certain engineering goals. A creative engineer
can use this information to construct a dynamic flow system for their own
needs.

So is the case with this book. The standard languages of RDF, RDFS, and
OWL provide the framework for the pieces an engineer can use to build a
model with dynamic behavior. Particular constructs like subClassOf and
subPropertyOf provide mechanisms for specifying how information flows
through the model. More advanced constructions like owl:Restriction
provide ways to specify complex relations between other parts of the
model. The examples from the ``in the wild'' chapters show how these
pieces have been assembled by working ontologists into complex dynamic
models that achieve particular goals. This is the craft of modeling in
the Semantic Web---combining the building blocks in useful ways to
create a dynamic system through which the data of the Semantic Web can
flow.

We have seen how the standards of the Semantic Web are already being used to manage data from 
a vast number of sources, and bring them together in a wide variety of applications.   This infrastructure 
of the Semantic Web has reached a level of maturity that allows many industries to rely
on distributed graph data systems
as the backbone for their long-term information systems. Search engines have 
embraced "knowledge graphs" that bring together all the information they 
know about a topic to enhance the services they can provide to their users.   
Now that the web of data is a reality and contains massive amounts of data, 

But data on the web is not just made up of high-value data sets curated by industrial sources. 
With the advent of social networks, data creation and consumption is performed by individuals 
on a massive, global scale.  The Semantic Web standards are equally applicable to this situation, 
in which millions of participants create tiny data contributions every minute of every day, all around
the world. 

In the current web, these contributions are managed by massive social network sites, who collect this data
and redistribute it to other users.  But they also amalgamate the data, and use it for various
big-data applications, such as product placement and advertising.  The users who contribute data
in this system give up rights to privacy and ownership of that data. Awareness of the dangers of such 
centralized control of data made it to the public
eye in the wake of high-profile uses of social data in the US Presidential Election of
2016 and the "Brexit" vote in the UK in the same year \cite{cambridge2018}. 


The application of the Semantic Web to social data is the basis of efforts like Solid (SOcial LInked Data)
\cite{berners2018one}, in which individual data providers, that is users of social networking platforms,
can manage their own data, while still allowing it to be integrated with other data for distributed use. 
There is a delicate balance between holding data private and allowing it to be shared, while still 
allowing the creator of the data to have control.  

This sort of balancing act, far from being a contradiction in terms, is exactly what we have come
to expect from a massive, distributed 
web of data.  Each data set its own entity, controlled by its owner or creator, and at the same time, it is a 
part of whole global network of data,  where each data set can interoperate with any other. 
But this  interoperation is not controlled by some master owner of the Web infrastrucutre, but by the owners of the
data.  This isn't a new idea on the Web; this is how it was designed to begin with, an how the Semantic Web can 
manage the massive information needs of the Web to come. 

